{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset and plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n",
    "def read_dataset(path,drop_columns=None,keep_columns=None):\n",
    "    #get rid of useless columns\n",
    "    csv_data = pd.read_csv(path)\n",
    "    \n",
    "    if keep_columns != None:\n",
    "        #keep only these columns\n",
    "        return csv_data.filter(items=keep_columns)\n",
    "    \n",
    "    if drop_columns!= None:\n",
    "        #drop these and keep the rest\n",
    "        return csv_data.drop(drop_columns, axis=1)\n",
    "    \n",
    "    #finally, didn't drop or filter any column\n",
    "    return csv_data     \n",
    "\n",
    "def plot_graph(g,ds_nodes=[],attribute_nodes=[],feat_nodes=[],lit_nodes=[]):\n",
    "    pos=nx.spring_layout(g)    \n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=ds_nodes,node_color=\"blue\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=attribute_nodes,node_color=\"green\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=feat_nodes,node_color=\"grey\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=lit_nodes,node_color=\"red\",node_size=900)\n",
    "\n",
    "    nx.draw_networkx_edges(g,pos,width=3)\n",
    "    nx.draw_networkx_labels(g,pos,font_size=8)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph  construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_ds_id(data):\n",
    "    return \"DS_\"+data\n",
    "def code_attr_id(data,parent):\n",
    "    return data+\"|\"+parent\n",
    "def code_feat_id(data,parent):\n",
    "    return data+\"|\"+parent\n",
    "def code_literal_id(data,parent):\n",
    "    return \"literal_\"+data+\"|\"+parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "    \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[1:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = instances\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        g.add_node(dataset_id,vector=word_embedding(\"dataset\",wem),tipo=\"dataset\")\n",
    "        row = datasets.iloc[r][1:]\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min(instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_dataset_id = code_feat_id(features[i],dataset_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),dataset_id)\n",
    "            g.add_node(feature_dataset_id,vector=word_embedding(\"feature dataset|\" +features[i] ,wem),tipo=\"feature dataset\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal dataset\")\n",
    "            g.add_edge(dataset_id,feature_dataset_id)\n",
    "            g.add_edge(feature_dataset_id,literal_dataset_id)\n",
    "            \n",
    "    return g\n",
    "\n",
    "\n",
    "def graph_attribute(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        attribute_id = code_attr_id(datasets.iloc[r][1],dataset_id)\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        g.add_node(attribute_id,vector=word_embedding(\"attribute\",wem),tipo=\"attribute\")\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "        g.add_edge(dataset_id,attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute_id = code_feat_id(features[i],attribute_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),attribute_id)\n",
    "            g.add_node(feature_attribute_id,vector=word_embedding(\"feature attribute|\"+features[i],wem),tipo=\"feature attribute\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal attribute\")\n",
    "            g.add_edge(attribute_id,feature_attribute_id)\n",
    "            g.add_edge(feature_attribute_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset_names(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "    \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = instances\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        g.add_node(dataset_id,vector=word_embedding(\"dataset|\"+datasets.iloc[r][1] ,wem),tipo=\"dataset\")\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min(instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_dataset_id = code_feat_id(features[i],dataset_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),dataset_id)\n",
    "            g.add_node(feature_dataset_id,vector=word_embedding(\"feature dataset|\" +features[i] ,wem),tipo=\"feature dataset\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal dataset\")\n",
    "            g.add_edge(dataset_id,feature_dataset_id)\n",
    "            g.add_edge(feature_dataset_id,literal_dataset_id)\n",
    "            \n",
    "    return g\n",
    "\n",
    "\n",
    "def graph_attribute_names(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        attribute_id = code_attr_id(datasets.iloc[r][1],dataset_id)\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        g.add_node(attribute_id,vector=word_embedding(\"attribute|\"+datasets.iloc[r][1],wem),tipo=\"attribute\")\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "        g.add_edge(dataset_id,attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute_id = code_feat_id(features[i],attribute_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),attribute_id)\n",
    "            g.add_node(feature_attribute_id,vector=word_embedding(\"feature attribute|\"+features[i],wem),tipo=\"feature attribute\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal attribute\")\n",
    "            g.add_edge(attribute_id,feature_attribute_id)\n",
    "            g.add_edge(feature_attribute_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset_short(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "    \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = instances\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        g.add_node(dataset_id,vector=word_embedding(\"dataset|\"+datasets.iloc[r][1] ,wem),tipo=\"dataset\")\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min(instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_dataset_id = code_feat_id(features[i],dataset_id)\n",
    "#             literal_dataset_id = code_literal_id(str(i),dataset_id)\n",
    "            g.add_node(feature_dataset_id,vector=word_embedding(features[i]+\"|\"+str(row[i]) ,wem),tipo=\"feature dataset\")\n",
    "#             g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal dataset\")\n",
    "            g.add_edge(dataset_id,feature_dataset_id)\n",
    "#             g.add_edge(feature_dataset_id,literal_dataset_id)\n",
    "            \n",
    "    return g\n",
    "\n",
    "\n",
    "def graph_attribute_short(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        attribute_id = code_attr_id(datasets.iloc[r][1],dataset_id)\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        g.add_node(attribute_id,vector=word_embedding(\"attribute|\"+datasets.iloc[r][1],wem),tipo=\"attribute\")\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "        g.add_edge(dataset_id,attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute_id = code_feat_id(features[i],attribute_id)\n",
    "#             literal_dataset_id = code_literal_id(str(i),attribute_id)\n",
    "            g.add_node(feature_attribute_id,vector=word_embedding(features[i]+\"|\"+str(row[i]),wem),tipo=\"feature attribute\")\n",
    "#             g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal attribute\")\n",
    "            g.add_edge(attribute_id,feature_attribute_id)\n",
    "#             g.add_edge(feature_attribute_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if input is number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    #Returns True is string is a number.\n",
    "    try:\n",
    "        float(s)\n",
    "        if float(s) == float(\"INF\"):\n",
    "            return False\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From numbers to bin tensor vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "import bitstring\n",
    "def num2vec(num):\n",
    "    \n",
    "    rep_sc = str('{:.11E}'.format(num))\n",
    "    \n",
    "    dec_part = int(rep_sc.split(\"E\")[0].replace(\".\",\"\"))\n",
    "    c = 1\n",
    "    if dec_part <0:\n",
    "        c = -1\n",
    "    dec_part = abs(dec_part)\n",
    "    \n",
    "    exp_part = int(rep_sc.split(\"E\")[1])\n",
    "    if exp_part <0:\n",
    "        exp_pos = 0\n",
    "        exp_neg = exp_part\n",
    "    else:\n",
    "        exp_pos = exp_part\n",
    "        exp_neg = 0\n",
    "\n",
    "    exp_pos = abs(exp_pos)    \n",
    "    exp_neg = abs(exp_neg)\n",
    "    \n",
    "    rep_str = str(\"{:03}{:03}{:012}\".format(exp_pos,exp_neg,dec_part))\n",
    "    \n",
    "#     print(dec_part)\n",
    "    rep_int = int(rep_str) * c\n",
    "    rep_bin = bitstring.Bits(int=rep_int, length=64).bin\n",
    "\n",
    "    bin_tensor = torch.tensor(np.array([float(x) for x in rep_bin]))\n",
    "    return bin_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import fasttext\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('./resources/fasttext.bin')\n",
    "print(ft.get_dimension())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttex_simple(value):\n",
    "    if is_number(value):\n",
    "        value = str(value)\n",
    "    \n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(300)\n",
    "    for v in values:\n",
    "        out_tensor = out_tensor + torch.tensor(ft.get_sentence_vector(value))\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor\n",
    "    \n",
    "def fasttex_(value):\n",
    "    value = str(value)\n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(364)\n",
    "    for v in values:\n",
    "        if is_number(v):\n",
    "            value_f = float(v)\n",
    "            bin_tensor = num2vec(value_f)\n",
    "            out_tensor = out_tensor + torch.cat((torch.zeros(300),bin_tensor.float()))\n",
    "        else:\n",
    "            str_tensor = torch.tensor(ft.get_sentence_vector(value))\n",
    "            out_tensor = out_tensor + torch.cat((str_tensor.float(),torch.zeros(64)))\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#pip install transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "#load model in memory\n",
    "tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "model = BertModel.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "def bert_simple(value):\n",
    "    if is_number(value):\n",
    "        value = str(value)\n",
    "    \n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(768)\n",
    "    for v in values:\n",
    "        #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "        tokenized = tokenizer.encode(v, add_special_tokens=True,max_length=512)\n",
    "        input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "        #result shape: (batch size, sequence length, model hidden dimension)\n",
    "#             print(last_hidden_states.shape)\n",
    "\n",
    "        #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "        out_tensor = out_tensor + torch.mean(last_hidden_states[0],dim=0).detach()\n",
    "\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor\n",
    "\n",
    "def bert(value):\n",
    "    value = str(value)\n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(832)\n",
    "    for v in values:\n",
    "        if is_number(v):\n",
    "            value = float(value)\n",
    "            bin_tensor = num2vec(value)\n",
    "            out_tensor = out_tensor + torch.cat((torch.zeros(768),bin_tensor.float()))\n",
    "        else:\n",
    "            #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "            tokenized = tokenizer.encode(v, add_special_tokens=True,max_length=512)\n",
    "            input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "            outputs = model(input_ids)\n",
    "            last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "            #result shape: (batch size, sequence length, model hidden dimension)\n",
    "#             print(last_hidden_states.shape)\n",
    "            #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "            str_tensor = torch.mean(last_hidden_states[0],dim=0).detach()\n",
    "            out_tensor = out_tensor + torch.cat((str_tensor.float(),torch.zeros(64)))\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(data, model):\n",
    "    if model==\"fasttext\":\n",
    "        return fasttex_(data)\n",
    "    if model==\"bert\":\n",
    "        return bert(data)\n",
    "    if model==\"fasttext_simple\":\n",
    "        return fasttex_simple(data)\n",
    "    if model==\"bert_simple\":\n",
    "        return bert_simple(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build graph\n",
    "word_emb = \"fasttext_simple\"\n",
    "df_dataset = read_dataset(\"./resources/openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset_short(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_short(df_attributes,g,word_emb)\n",
    "df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_short(df_attributes_numeric,g,word_emb)\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/encoded_\"+word_emb+\"_short.gpickle\")\n",
    "print(\"encoded_\"+word_emb+\"_short.gpickle\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build graph\n",
    "word_emb = \"fasttext\"\n",
    "df_dataset = read_dataset(\"./resources/openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset_short(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_short(df_attributes,g,word_emb)\n",
    "df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_short(df_attributes_numeric,g,word_emb)\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/encoded_\"+word_emb+\"2_short.gpickle\")\n",
    "print(\"encoded_\"+word_emb+\"2_short\")\n",
    "\n",
    "#build graph\n",
    "word_emb = \"fasttext\"\n",
    "df_dataset = read_dataset(\"./resources/openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute(df_attributes,g,word_emb)\n",
    "df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute(df_attributes_numeric,g,word_emb)\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/encoded_\"+word_emb+\"2.gpickle\")\n",
    "print(\"encoded_\"+word_emb+\"2_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_fasttext2_names\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#build graph\n",
    "word_emb = \"fasttext\"\n",
    "df_dataset = read_dataset(\"./resources/openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset_names(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_names(df_attributes,g,word_emb)\n",
    "df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_names(df_attributes_numeric,g,word_emb)\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/encoded_\"+word_emb+\"2_names.gpickle\")\n",
    "print(\"encoded_\"+word_emb+\"2_names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read previously created graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read\n",
    "g = nx.read_gpickle(\"../word_embeddings/encoded_bert_v2.gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [x for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "datasets_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature dataset\"]\n",
    "datasets_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal dataset\"]\n",
    "attributes = [x for x,y in g.nodes(data=True) if y['tipo']==\"attribute\"]\n",
    "attributes_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature attribute\"]\n",
    "attributes_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal attribute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(datasets))\n",
    "print(len(datasets_features))\n",
    "print(len(datasets_literals))\n",
    "print(len(attributes))\n",
    "print(len(attributes_features))\n",
    "print(len(attributes_literals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "datasets = [x for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "datasets_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature dataset\"]\n",
    "datasets_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal dataset\"]\n",
    "attributes = [x for x,y in g.nodes(data=True) if y['tipo']==\"attribute\"]\n",
    "attributes_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature attribute\"]\n",
    "attributes_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal attribute\"]\n",
    "# plot_graph(g,datasets,attributes,datasets_features + attributes_features,datasets_literals + attributes_literals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get nodes and print data from nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [y for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "print(sample[0][\"vector\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
