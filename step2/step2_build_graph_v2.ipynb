{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset and plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n",
    "def read_dataset(path,drop_columns=None,keep_columns=None):\n",
    "    #get rid of useless columns\n",
    "    csv_data = pd.read_csv(path)\n",
    "    \n",
    "    if keep_columns != None:\n",
    "        #keep only these columns\n",
    "        return csv_data.filter(items=keep_columns)\n",
    "    \n",
    "    if drop_columns!= None:\n",
    "        #drop these and keep the rest\n",
    "        return csv_data.drop(drop_columns, axis=1)\n",
    "    \n",
    "    #finally, didn't drop or filter any column\n",
    "    return csv_data     \n",
    "\n",
    "def plot_graph(g,ds_nodes=[],attribute_nodes=[],feat_nodes=[],lit_nodes=[]):\n",
    "    pos=nx.spring_layout(g)    \n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=ds_nodes,node_color=\"blue\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=attribute_nodes,node_color=\"green\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=feat_nodes,node_color=\"grey\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=lit_nodes,node_color=\"red\",node_size=900)\n",
    "\n",
    "    nx.draw_networkx_edges(g,pos,width=3)\n",
    "    nx.draw_networkx_labels(g,pos,font_size=8)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph  construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_ds_id(data):\n",
    "    return \"DS_\"+data\n",
    "def code_attr_id(data,parent):\n",
    "    return data+\"|\"+parent\n",
    "def code_feat_id(data,parent):\n",
    "    return data+\"|\"+parent\n",
    "def code_literal_id(data,parent):\n",
    "    return \"literal_\"+data+\"|\"+parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "    \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[1:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = instances\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        g.add_node(dataset_id,vector=word_embedding(\"dataset\",wem),tipo=\"dataset\")\n",
    "        row = datasets.iloc[r][1:]\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min(instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_dataset_id = code_feat_id(features[i],dataset_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),dataset_id)\n",
    "            g.add_node(feature_dataset_id,vector=word_embedding(\"feature dataset|\" +features[i] ,wem),tipo=\"feature dataset\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal dataset\")\n",
    "            g.add_edge(dataset_id,feature_dataset_id)\n",
    "            g.add_edge(feature_dataset_id,literal_dataset_id)\n",
    "            \n",
    "    return g\n",
    "\n",
    "\n",
    "def graph_attribute(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        attribute_id = code_attr_id(datasets.iloc[r][1],dataset_id)\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        g.add_node(attribute_id,vector=word_embedding(\"attribute\",wem),tipo=\"attribute\")\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "        g.add_edge(dataset_id,attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute_id = code_feat_id(features[i],attribute_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),attribute_id)\n",
    "            g.add_node(feature_attribute_id,vector=word_embedding(\"feature attribute|\"+features[i],wem),tipo=\"feature attribute\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal attribute\")\n",
    "            g.add_edge(attribute_id,feature_attribute_id)\n",
    "            g.add_edge(feature_attribute_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset_names(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "    \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = instances\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        g.add_node(dataset_id,vector=word_embedding(\"dataset|\"+datasets.iloc[r][1] ,wem),tipo=\"dataset\")\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min(instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_dataset_id = code_feat_id(features[i],dataset_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),dataset_id)\n",
    "            g.add_node(feature_dataset_id,vector=word_embedding(\"feature dataset|\" +features[i] ,wem),tipo=\"feature dataset\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal dataset\")\n",
    "            g.add_edge(dataset_id,feature_dataset_id)\n",
    "            g.add_edge(feature_dataset_id,literal_dataset_id)\n",
    "            \n",
    "    return g\n",
    "\n",
    "\n",
    "def graph_attribute_names(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        attribute_id = code_attr_id(datasets.iloc[r][1],dataset_id)\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        g.add_node(attribute_id,vector=word_embedding(\"attribute|\"+datasets.iloc[r][1],wem),tipo=\"attribute\")\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "        g.add_edge(dataset_id,attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute_id = code_feat_id(features[i],attribute_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),attribute_id)\n",
    "            g.add_node(feature_attribute_id,vector=word_embedding(\"feature attribute|\"+features[i],wem),tipo=\"feature attribute\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal attribute\")\n",
    "            g.add_edge(attribute_id,feature_attribute_id)\n",
    "            g.add_edge(feature_attribute_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset_short(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "    \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = instances\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        g.add_node(dataset_id,vector=word_embedding(\"dataset|\"+datasets.iloc[r][1] ,wem),tipo=\"dataset\")\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min(instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_dataset_id = code_feat_id(features[i],dataset_id)\n",
    "#             literal_dataset_id = code_literal_id(str(i),dataset_id)\n",
    "            g.add_node(feature_dataset_id,vector=word_embedding(features[i]+\"|\"+str(row[i]) ,wem),tipo=\"feature dataset\")\n",
    "#             g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal dataset\")\n",
    "            g.add_edge(dataset_id,feature_dataset_id)\n",
    "#             g.add_edge(feature_dataset_id,literal_dataset_id)\n",
    "            \n",
    "    return g\n",
    "\n",
    "\n",
    "def graph_attribute_short(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        attribute_id = code_attr_id(datasets.iloc[r][1],dataset_id)\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        g.add_node(attribute_id,vector=word_embedding(\"attribute|\"+datasets.iloc[r][1],wem),tipo=\"attribute\")\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "        g.add_edge(dataset_id,attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute_id = code_feat_id(features[i],attribute_id)\n",
    "#             literal_dataset_id = code_literal_id(str(i),attribute_id)\n",
    "            g.add_node(feature_attribute_id,vector=word_embedding(features[i]+\"|\"+str(row[i]),wem),tipo=\"feature attribute\")\n",
    "#             g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal attribute\")\n",
    "            g.add_edge(attribute_id,feature_attribute_id)\n",
    "#             g.add_edge(feature_attribute_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if input is number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    #Returns True is string is a number.\n",
    "    try:\n",
    "        float(s)\n",
    "        if float(s) == float(\"INF\") or float(s) == float(\"NAN\") or s == \"NAN\" or s == \"nan\":\n",
    "            return False\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def fill_ones(bin_rep,value=1,neg=1):\n",
    "    output = []\n",
    "#     neg = 10**neg\n",
    "    fill_one = False\n",
    "    for b in bin_rep:\n",
    "        if fill_one:\n",
    "            output.append(str(neg))\n",
    "#             output.append(str(value*neg/1000000))\n",
    "        else:\n",
    "            if b == \"1\":\n",
    "                output.append(str(neg))\n",
    "#                 output.append(str(value*neg/1000000))\n",
    "                fill_one=True\n",
    "            else:\n",
    "                output.append(\"0\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From numbers to bin tensor vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "import bitstring\n",
    "import torch\n",
    "\n",
    "def num2vec_old3(num):\n",
    "    rep_sc = str('{:.5E}'.format(num))\n",
    "#     print(rep_sc)\n",
    "    exp_part = int(rep_sc.split(\"E\")[1])\n",
    "    if exp_part <0:\n",
    "        exp_pos = 0\n",
    "        exp_neg = min(abs(exp_part)+1,2047)\n",
    "    else:\n",
    "        exp_pos = min(abs(exp_part)+1,2047)\n",
    "        exp_neg = 0\n",
    "\n",
    "    dec_part = rep_sc.split(\"E\")[0]\n",
    "    dec_part_int = dec_part.split(\".\")[0]\n",
    "    dec_part_frac = dec_part.split(\".\")[1][0:min(5,exp_pos + exp_neg)]#.rstrip('0')\n",
    "#     dec_part = int(dec_part_int)\n",
    "    dec_part = int(dec_part_int + dec_part_frac)\n",
    "#     dec_part = int(rep_sc.split(\"E\")[0].replace(\".\",\"\"))\n",
    "#     print(\"Exp+ :\" + str(exp_pos) + \" Exp- :\" + str(exp_neg) + \" Decimal:\"+ str(dec_part))\n",
    "    factor = 1\n",
    "    if dec_part <0:\n",
    "        factor = -1\n",
    "        dec_pos = 0\n",
    "        dec_neg = abs(dec_part)\n",
    "        neg_exp_pos = exp_pos\n",
    "        neg_exp_neg = exp_neg\n",
    "        pos_exp_pos = 0\n",
    "        pos_exp_neg = 0\n",
    "    else:\n",
    "        dec_pos = abs(dec_part)\n",
    "        dec_neg = 0    \n",
    "        neg_exp_pos = 0\n",
    "        neg_exp_neg = 0\n",
    "        pos_exp_pos = exp_pos\n",
    "        pos_exp_neg = exp_neg\n",
    "        \n",
    "    bin_pos_exp_pos = fill_ones(bitstring.Bits(uint=pos_exp_pos, length=11).bin)\n",
    "    bin_pos_exp_neg = fill_ones(bitstring.Bits(uint=pos_exp_neg, length=11).bin)\n",
    "    bin_neg_exp_pos = fill_ones(bitstring.Bits(uint=neg_exp_pos, length=11).bin)\n",
    "    bin_neg_exp_neg = fill_ones(bitstring.Bits(uint=neg_exp_neg, length=11).bin)\n",
    "    bin_dec = fill_ones(bitstring.Bits(uint=(dec_pos+dec_neg), length=20).bin,neg=factor)\n",
    "    \n",
    "    \n",
    "#     rep_str = str(\"{:03}{:03}{:03}{:03}\".format(pos_exp_pos,pos_exp_neg,neg_exp_pos,neg_exp_neg))\n",
    "#     rep_int_list = [int(char) for char in rep_str] \n",
    "    rep_bin = bin_pos_exp_pos + bin_pos_exp_neg + bin_neg_exp_pos + bin_neg_exp_neg + bin_dec\n",
    "#     bin_tensor = torch.tensor(np.array([float(x) for x in rep_bin]))\n",
    "    return bin_tensor\n",
    "\n",
    "\n",
    "def num2vec_old2(num):\n",
    "    rep_sc = str('{:.11E}'.format(num))\n",
    "#     print(rep_sc)\n",
    "    exp_part = int(rep_sc.split(\"E\")[1])\n",
    "    if exp_part <0:\n",
    "        exp_pos = 0\n",
    "        exp_neg = min(abs(exp_part)+1,1023)\n",
    "    else:\n",
    "        exp_pos = min(abs(exp_part)+1,1023)\n",
    "        exp_neg = 0\n",
    "\n",
    "    dec_part = rep_sc.split(\"E\")[0]\n",
    "    dec_part_int = dec_part.split(\".\")[0]\n",
    "    dec_part_frac = dec_part.split(\".\")[1][0:min(11,exp_pos + exp_neg)]#.rstrip('0')\n",
    "#     dec_part = int(dec_part_int)\n",
    "    dec_part = int(dec_part_int + dec_part_frac)\n",
    "#     dec_part = int(rep_sc.split(\"E\")[0].replace(\".\",\"\"))\n",
    "#     print(\"Exp+ :\" + str(exp_pos) + \" Exp- :\" + str(exp_neg) + \" Decimal:\"+ str(dec_part))\n",
    "    factor = 1\n",
    "    if dec_part <0:\n",
    "        factor = -1\n",
    "        \n",
    "    bin_exp_pos = fill_ones(bitstring.Bits(uint=exp_pos, length=11).bin)\n",
    "    bin_exp_neg = fill_ones(bitstring.Bits(uint=exp_neg, length=11).bin)\n",
    "    bin_dec = fill_ones(bitstring.Bits(uint=abs(dec_part), length=42).bin)\n",
    "    \n",
    "#     rep_str = str(\"{:03}{:03}{:03}{:03}\".format(pos_exp_pos,pos_exp_neg,neg_exp_pos,neg_exp_neg))\n",
    "#     rep_int_list = [int(char) for char in rep_str] \n",
    "    rep_bin = bin_exp_pos + bin_exp_neg + bin_dec\n",
    "    bin_tensor = torch.tensor(np.array([float(x)*factor for x in rep_bin]))\n",
    "    return bin_tensor\n",
    "\n",
    "def num2vec_old(num):\n",
    "    rep_sc = str('{:.3E}'.format(num))\n",
    "#     print(rep_sc)\n",
    "    exp_part = int(rep_sc.split(\"E\")[1])\n",
    "    if exp_part <0:\n",
    "        exp_pos = 0\n",
    "        exp_neg = min(abs(exp_part)+1,1023)\n",
    "    else:\n",
    "        exp_pos = min(abs(exp_part)+1,1023)\n",
    "        exp_neg = 0\n",
    "\n",
    "    dec_part = rep_sc.split(\"E\")[0]\n",
    "    dec_part_int = dec_part.split(\".\")[0]\n",
    "    dec_part_frac = dec_part.split(\".\")[1]#.rstrip('0')\n",
    "#     dec_part = int(dec_part_int)\n",
    "    dec_part = int(dec_part_int + dec_part_frac)\n",
    "#     dec_part = int(rep_sc.split(\"E\")[0].replace(\".\",\"\"))\n",
    "#     print(\"Exp+ :\" + str(exp_pos) + \" Exp- :\" + str(exp_neg) + \" Decimal:\"+ str(dec_part))\n",
    "    if dec_part <0:\n",
    "        dec_pos = 0\n",
    "        dec_neg = abs(dec_part)\n",
    "        neg_exp_pos = exp_pos\n",
    "        neg_exp_neg = exp_neg\n",
    "        pos_exp_pos = 0\n",
    "        pos_exp_neg = 0\n",
    "    else:\n",
    "        dec_pos = abs(dec_part)\n",
    "        dec_neg = 0    \n",
    "        neg_exp_pos = 0\n",
    "        neg_exp_neg = 0\n",
    "        pos_exp_pos = exp_pos\n",
    "        pos_exp_neg = exp_neg\n",
    "        \n",
    "    bin_pos_exp_pos = fill_ones(bitstring.Bits(uint=pos_exp_pos, length=10).bin,pos_exp_pos*1000000)\n",
    "    bin_pos_exp_neg = fill_ones(bitstring.Bits(uint=pos_exp_neg, length=10).bin,pos_exp_neg*1000000)\n",
    "    bin_neg_exp_pos = fill_ones(bitstring.Bits(uint=neg_exp_pos, length=10).bin,neg_exp_pos*1000000)\n",
    "    bin_neg_exp_neg = fill_ones(bitstring.Bits(uint=neg_exp_neg, length=10).bin,neg_exp_neg*1000000)\n",
    "    bin_dec_pos = fill_ones(bitstring.Bits(uint=(exp_pos+exp_neg), length=12).bin,dec_pos,exp_pos-exp_neg)\n",
    "    bin_dec_neg = fill_ones(bitstring.Bits(uint=(exp_pos+exp_neg), length=12).bin,dec_neg,exp_pos-exp_neg)\n",
    "\n",
    "#     rep_str = str(\"{:03}{:03}{:03}{:03}\".format(pos_exp_pos,pos_exp_neg,neg_exp_pos,neg_exp_neg))\n",
    "#     rep_int_list = [int(char) for char in rep_str] \n",
    "    rep_bin = bin_pos_exp_pos + bin_pos_exp_neg + bin_neg_exp_pos + bin_neg_exp_neg + bin_dec_pos + bin_dec_neg \n",
    "    bin_tensor = torch.tensor(np.array([float(x) for x in rep_bin]))\n",
    "    return bin_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2vec_just_old(num):\n",
    "    rep_sc = str('{:.8E}'.format(num))\n",
    "    print(rep_sc)\n",
    "    exp_part = int(rep_sc.split(\"E\")[1])\n",
    "    if exp_part <0:\n",
    "        exp_pos = 0\n",
    "        exp_neg = exp_part\n",
    "    else:\n",
    "        exp_pos = exp_part\n",
    "        exp_neg = 0\n",
    "\n",
    "    exp_pos = abs(exp_pos)    \n",
    "    exp_neg = abs(exp_neg)\n",
    "    \n",
    "    dec_part_int = rep_sc.split(\"E\")[0].split(\".\")[0]\n",
    "    dec_part_frac = rep_sc.split(\"E\")[0].split(\".\")[1][0:min(11,exp_pos+exp_neg)]\n",
    "    dec_part = int(dec_part_int+dec_part_frac)\n",
    "    c = 1.0\n",
    "    if dec_part <0:\n",
    "        c = -1.0\n",
    "    dec_part = abs(dec_part)\n",
    "    \n",
    "    rep_str = str(\"{:03}{:03}{:010}\".format(exp_pos,exp_neg,dec_part))\n",
    "    rep_final_str = []\n",
    "    previous = \"\"\n",
    "    for i in range (16):\n",
    "        dec  = rep_str[i]\n",
    "        for j in range(4):\n",
    "            rep_final_str.append(previous+dec)\n",
    "            \n",
    "        if i+1 < 6:\n",
    "            if (i+1) % 3 == 0:\n",
    "                previous = \"\"\n",
    "            else:\n",
    "                previous += dec\n",
    "        else:\n",
    "            previous = \"\"\n",
    "            \n",
    "        \n",
    "#     print(rep_final_str)    \n",
    "#     rep_str = rep_str+rep_str+rep_str+rep_str\n",
    "    rep_float = []\n",
    "    for i in range(64):\n",
    "        if i % 64 < 24:\n",
    "            if i % 24 < 4:\n",
    "                w = 1/10000\n",
    "            elif i % 24 < 8:\n",
    "                w = 1/1000\n",
    "            elif i%24 < 12:\n",
    "                w = 1/100\n",
    "            else:\n",
    "                w = 1/10\n",
    "        else:\n",
    "            w = 1\n",
    "        rep_float.append(float(rep_final_str[i])/w)\n",
    "#     print(\"rep str :\" + str(rep_str))\n",
    "#     print(dec_part)\n",
    "#     rep_int = int(rep_str)\n",
    "#     rep_bin = bitstring.Bits(uint=rep_int, length=64).bin\n",
    "#     print(rep_float)\n",
    "    bin_tensor = torch.tensor(np.array([x*c for x in rep_float]))\n",
    "#     print(bin_tensor)\n",
    "    return bin_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean\n",
    "def num2vec(num):\n",
    "    rep_sc = str('{:.11E}'.format(num))\n",
    "    print(rep_sc)\n",
    "    dec_part = int(rep_sc.split(\"E\")[0].replace(\".\",\"\"))\n",
    "    c = 1\n",
    "    if dec_part <0:\n",
    "        c = -1\n",
    "    dec_part = abs(dec_part)\n",
    "    \n",
    "    exp_part = int(rep_sc.split(\"E\")[1])\n",
    "    if exp_part <0:\n",
    "        exp_pos = 0\n",
    "        exp_neg = exp_part\n",
    "    else:\n",
    "        exp_pos = exp_part\n",
    "        exp_neg = 0\n",
    "\n",
    "    exp_pos = abs(exp_pos)    \n",
    "    exp_neg = abs(exp_neg)\n",
    "    \n",
    "    rep_str = str(\"{:03}{:03}{:012}\".format(exp_pos,exp_neg,dec_part))\n",
    "    print(rep_str)\n",
    "    \n",
    "#     print(dec_part)\n",
    "    rep_int = int(rep_str) * c\n",
    "    rep_bin = bitstring.Bits(int=rep_int, length=64).bin\n",
    "\n",
    "    bin_tensor = torch.tensor(np.array([float(x) for x in rep_bin]))\n",
    "    return bin_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "##clean_5\n",
    "def num2vec_clean5(num):\n",
    "    rep_sc = str('{:.5E}'.format(num))\n",
    "    exp_part = int(rep_sc.split(\"E\")[1])\n",
    "    if exp_part <0:\n",
    "        exp_pos = 0\n",
    "        exp_neg = exp_part\n",
    "    else:\n",
    "        exp_pos = exp_part\n",
    "        exp_neg = 0\n",
    "\n",
    "    exp_pos = abs(exp_pos)    \n",
    "    exp_neg = abs(exp_neg)\n",
    "    \n",
    "    dec_part = rep_sc.split(\"E\")[0]\n",
    "    c = 1.0\n",
    "    if float(dec_part) <0:\n",
    "        c = -1.0\n",
    "    float_dec = abs(float(dec_part))\n",
    "    \n",
    "    final_rep = []\n",
    "    for i in range (64):\n",
    "        if i < 22:\n",
    "            final_rep.append(exp_pos/22)\n",
    "        elif i < 44:\n",
    "            final_rep.append(exp_neg/22)\n",
    "        else:\n",
    "            final_rep.append(float_dec/200)\n",
    "            \n",
    "    bin_tensor = torch.tensor(np.array([float(x)*c for x in final_rep]))\n",
    "    return bin_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "##clean6\n",
    "def num2vec_clean6(num):\n",
    "    rep_sc = str('{:.2E}'.format(num))\n",
    "    print(rep_sc)\n",
    "    exp_part = int(rep_sc.split(\"E\")[1])\n",
    "    if exp_part <0:\n",
    "        exp_pos = 0\n",
    "        exp_neg = exp_part\n",
    "    else:\n",
    "        exp_pos = exp_part\n",
    "        exp_neg = 0\n",
    "\n",
    "    exp_pos = abs(exp_pos)\n",
    "    exp_neg = abs(exp_neg)\n",
    "    \n",
    "    dec_part_int = rep_sc.split(\"E\")[0].split(\".\")[0]\n",
    "    dec_part_frac = rep_sc.split(\"E\")[0].split(\".\")[1][0:min(2,(exp_pos+exp_neg))]\n",
    "    dec_part = int(dec_part_int+dec_part_frac)\n",
    "    print(dec_part)\n",
    "    c = 1\n",
    "    if dec_part <0:\n",
    "        c = -1\n",
    "    dec_part = abs(dec_part)\n",
    "    \n",
    "    if exp_pos >=0:\n",
    "        rep_exp_pos = int(str(\"{:03}{:03}\".format(exp_pos,dec_part)))\n",
    "    else:\n",
    "        rep_exp_pos = 0\n",
    "    if exp_neg >=0:\n",
    "        rep_exp_neg = int(str(\"{:03}{:03}\".format(exp_neg,dec_part)))\n",
    "    else:\n",
    "        rep_exp_neg = 0 \n",
    "    rep_dec_str = int(str(\"{:03}\".format(dec_part)))\n",
    "    \n",
    "    \n",
    "    num_value = (exp_pos+exp_neg)/10 + dec_part/10\n",
    "    float_rep = []\n",
    "    if c < 1:\n",
    "        for i in range(32):\n",
    "            float_rep.append(0)\n",
    "        for i in range(16):\n",
    "            float_rep.append(rep_exp_pos/1000)\n",
    "        for i in range(16):\n",
    "            float_rep.append(rep_exp_neg/1000)\n",
    "#         for i in range(10):\n",
    "#             float_rep.append(rep_dec_str/1000)\n",
    "            \n",
    "    else:\n",
    "        for i in range(16):\n",
    "            float_rep.append(rep_exp_pos/1000)\n",
    "        for i in range(16):\n",
    "            float_rep.append(rep_exp_neg/1000)\n",
    "#         for i in range(10):\n",
    "#             float_rep.append(rep_dec_str/1000)\n",
    "        for i in range(32):\n",
    "            float_rep.append(0)    \n",
    "#     if c < 1:\n",
    "#         for i in range(32):\n",
    "#             float_rep.append(0)\n",
    "#         for i in range(10):\n",
    "#             float_rep.append(exp_pos/10)\n",
    "#         for i in range(10):\n",
    "#             float_rep.append(exp_neg/10)\n",
    "#         for i in range(12):\n",
    "#             float_rep.append(dec_part/100)\n",
    "            \n",
    "#     else:\n",
    "#         for i in range(10):\n",
    "#             float_rep.append(exp_pos/10)\n",
    "#         for i in range(10):\n",
    "#             float_rep.append(exp_neg/10)\n",
    "#         for i in range(12):\n",
    "#             float_rep.append(dec_part/100)\n",
    "#         for i in range(32):\n",
    "#             float_rep.append(0)\n",
    "\n",
    "    bin_tensor = torch.tensor(np.array([float(x) for x in float_rep]))\n",
    "    return bin_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00000000000E-08\n",
      "000008100000000000\n",
      "1111111111111111111110001010001000010010011000001001100000000000\n",
      "-2.77665540000E+300\n",
      "300000277665540000\n",
      "1111101111010110001011110101011001000001001110100001100001100000\n",
      "tensor(0.5806)\n",
      "tensor(5.0990)\n"
     ]
    }
   ],
   "source": [
    "import bitstring\n",
    "import torch\n",
    "import numpy as np\n",
    "nums = [-0.00000001,-2.7766554*10**300]\n",
    "tensors = []\n",
    "for n in nums:\n",
    "    tensors.append(num2vec(n))\n",
    "#     print(tensors[-1])\n",
    "    vec = tensors[-1].tolist()\n",
    "    output = \"\"\n",
    "    for v in vec:\n",
    "        output = output + str(int(v))\n",
    "    print(output)\n",
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "result = cos(tensors[0],tensors[1])\n",
    "print(result.float())\n",
    "pdist = torch.nn.PairwiseDistance(p=2)\n",
    "norm_euclidean = pdist(torch.stack([tensors[0]]),torch.stack([tensors[1]])) \n",
    "print(norm_euclidean[0].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0011111111100001100110011001100110011001100110011001100110011010\n",
      "0100000011101010110110110000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "b1 = bitstring.Bits(float=nums[0], length=64).bin\n",
    "b2 = bitstring.Bits(float=nums[1], length=64).bin\n",
    "print(b1)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import fasttext\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('./resources/fasttext.bin')\n",
    "print(ft.get_dimension())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttex_simple(value):\n",
    "    if is_number(value):\n",
    "        value = str(value)\n",
    "    \n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(300)\n",
    "    for v in values:\n",
    "        out_tensor = out_tensor + torch.tensor(ft.get_sentence_vector(value))\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor\n",
    "    \n",
    "def fasttex_(value):\n",
    "    value = str(value)\n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(364)\n",
    "    for v in values:\n",
    "        if is_number(v):\n",
    "            value_f = float(v)\n",
    "            bin_tensor = num2vec(value_f)\n",
    "            out_tensor = out_tensor + torch.cat((torch.zeros(300),bin_tensor.float()))\n",
    "        else:\n",
    "            str_tensor = torch.tensor(ft.get_sentence_vector(value))\n",
    "            out_tensor = out_tensor + torch.cat((str_tensor.float(),torch.zeros(64)))\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#pip install transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "#load model in memory\n",
    "tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "model = BertModel.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "def bert_simple(value):\n",
    "    if is_number(value):\n",
    "        value = str(value)\n",
    "    \n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(768)\n",
    "    for v in values:\n",
    "        #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "        tokenized = tokenizer.encode(v, add_special_tokens=True,max_length=512)\n",
    "        input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "        #result shape: (batch size, sequence length, model hidden dimension)\n",
    "#             print(last_hidden_states.shape)\n",
    "\n",
    "        #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "        out_tensor = out_tensor + torch.mean(last_hidden_states[0],dim=0).detach()\n",
    "\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor\n",
    "\n",
    "def bert(value):\n",
    "    value = str(value)\n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(832)\n",
    "    for v in values:\n",
    "        if is_number(v):\n",
    "            v_f = float(v)\n",
    "            bin_tensor = num2vec(v_f)\n",
    "            out_tensor = out_tensor + torch.cat((torch.zeros(768),bin_tensor.float()))\n",
    "        else:\n",
    "            #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "            tokenized = tokenizer.encode(v, add_special_tokens=True,max_length=512)\n",
    "            input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "            outputs = model(input_ids)\n",
    "            last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "            #result shape: (batch size, sequence length, model hidden dimension)\n",
    "#             print(last_hidden_states.shape)\n",
    "            #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "            str_tensor = torch.mean(last_hidden_states[0],dim=0).detach()\n",
    "            out_tensor = out_tensor + torch.cat((str_tensor.float(),torch.zeros(64)))\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(data, model):\n",
    "    if model==\"fasttext\":\n",
    "        return fasttex_(data)\n",
    "    if model==\"bert\":\n",
    "        return bert(data)\n",
    "    if model==\"fasttext_simple\":\n",
    "        return fasttex_simple(data)\n",
    "    if model==\"bert_simple\":\n",
    "        return bert_simple(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_fasttext_simple_short.gpickle\n"
     ]
    }
   ],
   "source": [
    "#build graph\n",
    "word_emb = \"fasttext\"\n",
    "df_dataset = read_dataset(\"./resources/monitor_clean/ds2.csv\");\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset_short(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/monitor_clean/attr_cat2.csv\");\n",
    "g = graph_attribute_short(df_attributes,g,word_emb)\n",
    "df_attributes_numeric = read_dataset(\"./resources/monitor_clean/attr_num2.csv\");\n",
    "g = graph_attribute_short(df_attributes_numeric,g,word_emb)\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/clean_monitor_\"+word_emb+\".gpickle\")\n",
    "print(\"clean_monitor\"+word_emb+\".gpickle\")\n",
    "##clean with older notation\n",
    "##clean 2 with higher numbers that didnt work\n",
    "##clean 3 with exp+,exp-,fraction\n",
    "##clean 4 with +exp+,+exp-,-exp+,-exp-,fraction with sign\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(g, \"../word_embeddings/new_\"+word_emb+\"_short.gpickle\")\n",
    "print(\"new_\"+word_emb+\"_short.gpickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build graph\n",
    "word_emb = \"fasttext\"\n",
    "df_dataset = read_dataset(\"./resources/openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset_short(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_short(df_attributes,g,word_emb)\n",
    "df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_short(df_attributes_numeric,g,word_emb)\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/encoded_\"+word_emb+\"2_short.gpickle\")\n",
    "print(\"encoded_\"+word_emb+\"2_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build graph\n",
    "word_emb = \"fasttext\"\n",
    "df_dataset = read_dataset(\"./resources/openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset_short(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_short(df_attributes,g,word_emb)\n",
    "df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_short(df_attributes_numeric,g,word_emb)\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/encoded_\"+word_emb+\"2_short.gpickle\")\n",
    "print(\"encoded_\"+word_emb+\"2_short\")\n",
    "\n",
    "#build graph\n",
    "word_emb = \"fasttext\"\n",
    "df_dataset = read_dataset(\"./resources/openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute(df_attributes,g,word_emb)\n",
    "df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute(df_attributes_numeric,g,word_emb)\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/encoded_\"+word_emb+\"2.gpickle\")\n",
    "print(\"encoded_\"+word_emb+\"2_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#build graph\n",
    "word_emb = \"fasttext\"\n",
    "df_dataset = read_dataset(\"./resources/openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset_names(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_names(df_attributes,g,word_emb)\n",
    "df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute_names(df_attributes_numeric,g,word_emb)\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/encoded_\"+word_emb+\"2_names.gpickle\")\n",
    "print(\"encoded_\"+word_emb+\"2_names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read previously created graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read\n",
    "g = nx.read_gpickle(\"../word_embeddings/encoded_bert_v2.gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [x for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "datasets_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature dataset\"]\n",
    "datasets_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal dataset\"]\n",
    "attributes = [x for x,y in g.nodes(data=True) if y['tipo']==\"attribute\"]\n",
    "attributes_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature attribute\"]\n",
    "attributes_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal attribute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(datasets))\n",
    "print(len(datasets_features))\n",
    "print(len(datasets_literals))\n",
    "print(len(attributes))\n",
    "print(len(attributes_features))\n",
    "print(len(attributes_literals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "datasets = [x for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "datasets_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature dataset\"]\n",
    "datasets_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal dataset\"]\n",
    "attributes = [x for x,y in g.nodes(data=True) if y['tipo']==\"attribute\"]\n",
    "attributes_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature attribute\"]\n",
    "attributes_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal attribute\"]\n",
    "# plot_graph(g,datasets,attributes,datasets_features + attributes_features,datasets_literals + attributes_literals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get nodes and print data from nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [y for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "print(sample[0][\"vector\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
