{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset and plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n",
    "def read_dataset(path,drop_columns=None,keep_columns=None):\n",
    "    #get rid of useless columns\n",
    "    csv_data = pd.read_csv(path,sep='|')\n",
    "    \n",
    "    if keep_columns != None:\n",
    "        #keep only these columns\n",
    "        return csv_data.filter(items=keep_columns)\n",
    "    \n",
    "    if drop_columns!= None:\n",
    "        #drop these and keep the rest\n",
    "        return csv_data.drop(drop_columns, axis=1)\n",
    "    \n",
    "    #finally, didn't drop or filter any column\n",
    "    return csv_data     \n",
    "\n",
    "def plot_graph(g,ds_nodes=[],attribute_nodes=[],feat_nodes=[],lit_nodes=[]):\n",
    "    pos=nx.spring_layout(g)    \n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=ds_nodes,node_color=\"blue\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=attribute_nodes,node_color=\"green\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=feat_nodes,node_color=\"grey\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=lit_nodes,node_color=\"red\",node_size=900)\n",
    "\n",
    "    nx.draw_networkx_edges(g,pos,width=3)\n",
    "    nx.draw_networkx_labels(g,pos,font_size=8)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph  construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_ds_id(data):\n",
    "    return \"DS_\"+data\n",
    "def code_attr_id(data,parent):\n",
    "    return data+\"|\"+parent\n",
    "def code_feat_id(data,parent):\n",
    "    return data+\"|\"+parent\n",
    "def code_literal_id(data,parent):\n",
    "    return \"literal_\"+data+\"|\"+parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "    \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[1:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = instances\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        g.add_node(dataset_id,vector=word_embedding(\"dataset\",wem),tipo=\"dataset\")\n",
    "        row = datasets.iloc[r][1:]\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min(instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_dataset_id = code_feat_id(features[i],dataset_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),dataset_id)\n",
    "            g.add_node(feature_dataset_id,vector=word_embedding(\"feature dataset|\" +features[i] ,wem),tipo=\"feature dataset\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal dataset\")\n",
    "            g.add_edge(dataset_id,feature_dataset_id)\n",
    "            g.add_edge(feature_dataset_id,literal_dataset_id)\n",
    "            \n",
    "    return g\n",
    "\n",
    "\n",
    "def graph_attribute(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[1:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "#         attribute_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        attribute_id = \"DS_\"+datasets.iloc[r][0]\n",
    "        row = datasets.iloc[r][1:]\n",
    "        \n",
    "        g.add_node(attribute_id,vector=word_embedding(attribute_id,wem),tipo=\"dataset\")\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "#         g.add_edge(dataset_id,attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute_id = code_feat_id(features[i],attribute_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),attribute_id)\n",
    "            g.add_node(feature_attribute_id,vector=word_embedding(\"feature attribute|\"+features[i],wem),tipo=\"feature attribute\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal attribute\")\n",
    "            g.add_edge(attribute_id,feature_attribute_id)\n",
    "            g.add_edge(feature_attribute_id,literal_dataset_id)\n",
    "            \n",
    "    return g\n",
    "\n",
    "def graph_attribute_short(datasets,g=None,wem=\"fasttext\",instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[1:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "#         attribute_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        attribute_id = \"DS_\"+datasets.iloc[r][0]\n",
    "        row = datasets.iloc[r][1:]\n",
    "        \n",
    "        g.add_node(attribute_id,vector=word_embedding(attribute_id,wem),tipo=\"dataset\")\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "#         g.add_edge(dataset_id,attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute_id = code_feat_id(features[i],attribute_id)\n",
    "#             literal_dataset_id = code_literal_id(str(i),attribute_id)\n",
    "            g.add_node(feature_attribute_id,vector=word_embedding(features[i]+\"|\"+str(row[i]),wem),tipo=\"feature attribute\")\n",
    "#             g.add_node(literal_dataset_id,vector=word_embedding(row[i],wem),tipo=\"literal attribute\")\n",
    "            g.add_edge(attribute_id,feature_attribute_id)\n",
    "#             g.add_edge(feature_attribute_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if input is number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    #Returns True is string is a number.\n",
    "    try:\n",
    "        float(s)\n",
    "        if float(s) == float(\"INF\"):\n",
    "            return False\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From numbers to bin tensor vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "import bitstring\n",
    "def num2vec(num):\n",
    "    \n",
    "    rep_sc = str('{:.11E}'.format(num))\n",
    "    \n",
    "    dec_part = int(rep_sc.split(\"E\")[0].replace(\".\",\"\"))\n",
    "    c = 1\n",
    "    if dec_part <0:\n",
    "        c = -1\n",
    "    dec_part = abs(dec_part)\n",
    "    \n",
    "    exp_part = int(rep_sc.split(\"E\")[1])\n",
    "    if exp_part <0:\n",
    "        exp_pos = 0\n",
    "        exp_neg = exp_part\n",
    "    else:\n",
    "        exp_pos = exp_part\n",
    "        exp_neg = 0\n",
    "\n",
    "    exp_pos = abs(exp_pos)    \n",
    "    exp_neg = abs(exp_neg)\n",
    "    \n",
    "    rep_str = str(\"{:03}{:03}{:012}\".format(exp_pos,exp_neg,dec_part))\n",
    "    \n",
    "#     print(dec_part)\n",
    "    rep_int = int(rep_str) * c\n",
    "    rep_bin = bitstring.Bits(int=rep_int, length=64).bin\n",
    "\n",
    "    bin_tensor = torch.tensor(np.array([float(x) for x in rep_bin]))\n",
    "    return bin_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import fasttext\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('./resources/fasttext.bin')\n",
    "print(ft.get_dimension())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttex_simple(value):\n",
    "    if is_number(value):\n",
    "        value = str(value)\n",
    "    \n",
    "    value = value.replace(\"\\n\",\" \")\n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(300)\n",
    "    for v in values:\n",
    "        out_tensor = out_tensor + torch.tensor(ft.get_sentence_vector(value))\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor\n",
    "    \n",
    "def fasttex_(value):\n",
    "    value = str(value)\n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(364)\n",
    "    for v in values:\n",
    "        if is_number(v):\n",
    "            value_f = float(v)\n",
    "            bin_tensor = num2vec(value_f)\n",
    "            out_tensor = out_tensor + torch.cat((torch.zeros(300),bin_tensor.float()))\n",
    "        else:\n",
    "            str_tensor = torch.tensor(ft.get_sentence_vector(value))\n",
    "            out_tensor = out_tensor + torch.cat((str_tensor.float(),torch.zeros(64)))\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#pip install transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "#load model in memory\n",
    "tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "model = BertModel.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "def bert_simple(value):\n",
    "    if is_number(value):\n",
    "        value = str(value)\n",
    "    value = value.replace(\"\\n\",\" \")\n",
    "    values = value.split(\"|\")\n",
    "    out_tensor = torch.zeros(768)\n",
    "    for v in values:\n",
    "        #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "        tokenized = tokenizer.encode(v, add_special_tokens=True,max_length=512)\n",
    "        input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "        #result shape: (batch size, sequence length, model hidden dimension)\n",
    "#             print(last_hidden_states.shape)\n",
    "\n",
    "        #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "        out_tensor = out_tensor + torch.mean(last_hidden_states[0],dim=0).detach()\n",
    "\n",
    "    out_tensor = out_tensor / len(values)\n",
    "    return out_tensor\n",
    "\n",
    "def bert(value):\n",
    "    if is_number(value):\n",
    "        value = float(value)\n",
    "        bin_tensor = num2vec(value)\n",
    "        return torch.cat((torch.zeros(768),bin_tensor.float()))\n",
    "    else:\n",
    "        values = value.split(\"|\")\n",
    "        out_tensor = torch.zeros(768)\n",
    "        for v in values:\n",
    "            #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "            tokenized = tokenizer.encode(v, add_special_tokens=True,max_length=512)\n",
    "            input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "            last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "            #result shape: (batch size, sequence length, model hidden dimension)\n",
    "#             print(last_hidden_states.shape)\n",
    "\n",
    "            #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "            out_tensor = out_tensor + torch.mean(last_hidden_states[0],dim=0).detach()\n",
    "            \n",
    "        out_tensor = out_tensor / len(values)\n",
    "        return torch.cat((out_tensor.float(),torch.zeros(64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(data, model):\n",
    "    if model==\"fasttext\":\n",
    "        return fasttex_(data)\n",
    "    if model==\"bert\":\n",
    "        return bert(data)\n",
    "    if model==\"fasttext_simple\":\n",
    "        return fasttex_simple(data)\n",
    "    if model==\"bert_simple\":\n",
    "        return bert_simple(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build graph\n",
    "word_emb = \"fasttext_simple\"\n",
    "# df_dataset = read_dataset(\"./resources/monitor_nominal.csv\",drop_columns=[\"value_counts_with_nan\", \"value_counts_without_nan\",\"distinct_count_with_nan\",\"distinct_count_without_nan\",\"n\"\n",
    "#                                                                           ,\"p_infinite\",\"n_infinite\",\"date_warning\",\"histogram_data\",\"scatter_data\"]);\n",
    "g = g = nx.Graph()\n",
    "# g = graph_dataset(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/monitor_nominal.csv\",drop_columns=[\"value_counts_with_nan\", \"value_counts_without_nan\",\"distinct_count_with_nan\",\"distinct_count_without_nan\",\"n\",\"p_infinite\",\"n_infinite\",\"date_warning\",\"histogram_data\",\"scatter_data\"])\n",
    "g = graph_attribute_short(df_attributes,g,word_emb)\n",
    "# df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "# g = graph_attribute(df_attributes_numeric,g,word_emb)\n",
    "\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/monitor_\"+word_emb+\"_short.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5f11be97b2b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# g = graph_dataset(df_dataset,g,word_emb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./resources/monitor_nominal.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value_counts_with_nan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value_counts_without_nan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"distinct_count_with_nan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"distinct_count_without_nan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"p_infinite\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"n_infinite\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"date_warning\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"histogram_data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"scatter_data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# g = graph_attribute(df_attributes_numeric,g,word_emb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-17b1ede25472>\u001b[0m in \u001b[0;36mgraph_attribute\u001b[0;34m(datasets, g, wem, instances)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mliteral_dataset_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode_literal_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattribute_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_attribute_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feature attribute|\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtipo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"feature attribute\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliteral_dataset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtipo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"literal attribute\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_attribute_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_attribute_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mliteral_dataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-801e27b3c397>\u001b[0m in \u001b[0;36mword_embedding\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfasttex_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"bert_simple\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbert_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-d20cea5db2b7>\u001b[0m in \u001b[0;36mbert_simple\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Batch size 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# The last hidden-state is the first element of the output tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         )\n\u001b[1;32m    792\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m     ):\n\u001b[1;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#build graph\n",
    "word_emb = \"fasttext_simple\"\n",
    "# df_dataset = read_dataset(\"./resources/monitor_nominal.csv\",drop_columns=[\"value_counts_with_nan\", \"value_counts_without_nan\",\"distinct_count_with_nan\",\"distinct_count_without_nan\",\"n\"\n",
    "#                                                                           ,\"p_infinite\",\"n_infinite\",\"date_warning\",\"histogram_data\",\"scatter_data\"]);\n",
    "g = g = nx.Graph()\n",
    "# g = graph_dataset(df_dataset,g,word_emb)\n",
    "df_attributes = read_dataset(\"./resources/monitor_nominal.csv\",drop_columns=[\"value_counts_with_nan\", \"value_counts_without_nan\",\"distinct_count_with_nan\",\"distinct_count_without_nan\",\"n\",\"p_infinite\",\"n_infinite\",\"date_warning\",\"histogram_data\",\"scatter_data\"])\n",
    "g = graph_attribute(df_attributes,g,word_emb)\n",
    "# df_attributes_numeric = read_dataset(\"./resources/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "# g = graph_attribute(df_attributes_numeric,g,word_emb)\n",
    "\n",
    "#write graph to file\n",
    "nx.write_gpickle(g, \"../word_embeddings/monitor_\"+word_emb+\".gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read previously created graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read\n",
    "g = nx.read_gpickle(\"../word_embeddings/monitor_fasttext_simple.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS_www.cleverboxes.com//mount type\n"
     ]
    }
   ],
   "source": [
    "datasets = [x for x,y in g.nodes(data=True) if \"DS_www.cleverboxes.com//mount type\" == x.strip() ]\n",
    "for d in datasets:\n",
    "    print (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(\"hola \".strip())==\"hola\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "d = [x for x in datasets if x==\"www.cleverboxes.com//mount_type\"]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'DS_www.cleverboxes.com//mount_type' == 'DS_www.cleverboxes.com//mount_type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bf96018e9790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tipo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdatasets_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tipo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"feature dataset\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdatasets_literals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tipo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"literal dataset\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tipo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mattributes_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tipo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"feature attribute\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "datasets = [x for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "datasets_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature dataset\"]\n",
    "datasets_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal dataset\"]\n",
    "attributes = [x for x,y in g.nodes(data=True) if y['tipo']==\"attribute\"]\n",
    "attributes_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature attribute\"]\n",
    "attributes_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal attribute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(datasets))\n",
    "print(len(datasets_features))\n",
    "print(len(datasets_literals))\n",
    "print(len(attributes))\n",
    "print(len(attributes_features))\n",
    "print(len(attributes_literals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "datasets = [x for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "datasets_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature dataset\"]\n",
    "datasets_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal dataset\"]\n",
    "attributes = [x for x,y in g.nodes(data=True) if y['tipo']==\"attribute\"]\n",
    "attributes_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature attribute\"]\n",
    "attributes_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal attribute\"]\n",
    "# plot_graph(g,datasets,attributes,datasets_features + attributes_features,datasets_literals + attributes_literals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get nodes and print data from nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [y for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "print(sample[0][\"vector\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
