{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "\n",
    "ds_set = []\n",
    "ds_header = [\"ds_id\",\"ds_name\"]\n",
    "att_cat_set = []\n",
    "att_cat_header = [\"ds_id\",\"att_name\"]\n",
    "att_num_set = []\n",
    "att_num_header = [\"ds_id\",\"att_name\"]\n",
    "\n",
    "ds_data = None\n",
    "ds_names = []\n",
    "\n",
    "set_ds = []\n",
    "set_at_num = []\n",
    "set_at_cat = []\n",
    "\n",
    "final_ds_set = []\n",
    "final_nom_set = []\n",
    "final_num_set = []\n",
    "final_ds_set_header = []\n",
    "final_nom_set_header = []\n",
    "final_num_set_header = []\n",
    "\n",
    "#get dirs to read files\n",
    "def get_dirs(path):\n",
    "    drs = os.walk(path)\n",
    "    dirs = []\n",
    "    for dr in drs:\n",
    "        dirs.append(dr)\n",
    "    return dirs[1:]\n",
    "\n",
    "def concat_values(json_data):\n",
    "    for key in json_data.keys():\n",
    "        if key == \"value_counts\" or key == \"types\":\n",
    "            concat = \"\"\n",
    "            for k in json_data[key].keys():\n",
    "                if concat != \"\":\n",
    "                    concat = concat + \"|\"\n",
    "                concat = concat + str(k) + \" \" + str(json_data[key][k])\n",
    "            json_data[key] = concat\n",
    "    return json_data\n",
    "\n",
    "def write_csv(dirs):\n",
    "    df = None\n",
    "    for d in range(len(dirs)):\n",
    "        att_list = []\n",
    "        csv = []\n",
    "        #read file\n",
    "        for f in dirs[d][2]:\n",
    "            web_site = dirs[d][0].split(\"/\")[-1]\n",
    "            ds_names.append(web_site)\n",
    "            path = dirs[d][0]+\"/\"+f\n",
    "            df = pd.read_csv(path,sep=\"~\", error_bad_lines=False)\n",
    "            if len(df) > 10000:\n",
    "                df = df.sample(10000)\n",
    "        #get profile inf\n",
    "        profile = ProfileReport(df, minimal=True)\n",
    "        json_data = profile.to_json()\n",
    "        mf_json = json.loads(json_data)\n",
    "        \n",
    "        ds_mf = mf_json[\"table\"]\n",
    "        ds_mf[\"name\"] = ds_names[d]\n",
    "        print(ds_names[d])\n",
    "        ds_mf[\"nominal\"] = {}\n",
    "        ds_mf[\"numeric\"] = {}\n",
    "        set_ds.append(ds_mf)\n",
    "        variables = mf_json[\"variables\"] \n",
    "        for key in variables.keys():\n",
    "            at_mf = variables[key]\n",
    "            at_mf[\"ds_id\"] = d \n",
    "            if at_mf[\"type\"]==\"Variable.TYPE_NUM\":\n",
    "                del at_mf[\"histogram_data\"]\n",
    "                del at_mf[\"scatter_data\"]\n",
    "                del at_mf[\"histogram_bins\"]\n",
    "                ds_mf[\"numeric\"][key] = at_mf \n",
    "            else:\n",
    "                ds_mf[\"nominal\"][key] = at_mf\n",
    "                \n",
    "        att_numeric = ds_mf.pop(\"numeric\")\n",
    "        att_cat = ds_mf.pop(\"nominal\")\n",
    "#         data = concat_values(ds_mf)\n",
    "        data = ds_mf\n",
    "        del data[\"CAT\"]\n",
    "        del data[\"BOOL\"]\n",
    "        del data[\"NUM\"]\n",
    "        del data[\"DATE\"]\n",
    "        del data[\"URL\"]\n",
    "        del data[\"COMPLEX\"]\n",
    "        del data[\"PATH\"]\n",
    "        del data[\"FILE\"]\n",
    "        del data[\"IMAGE\"]\n",
    "        del data[\"UNSUPPORTED\"]\n",
    "        data_final = {}\n",
    "        data_final[\"ds_id\"] = data[\"name\"]\n",
    "        data_final[\"dataset name\"] = data[\"name\"]\n",
    "\n",
    "        ds_row = []\n",
    "        final_ds_row = []\n",
    "\n",
    "        ds_row.append(d)\n",
    "        ds_row.append(data[\"name\"])\n",
    "        \n",
    "        if len(ds_header) <= 2:\n",
    "            for key in data.keys():\n",
    "                ds_header.append(key)\n",
    "        for key in data.keys():\n",
    "            ds_row.append(data[key])\n",
    "        ds_set.append(ds_row)\n",
    "\n",
    "        ##data according alsefari\n",
    "        data_final[\"number of instances\"] = data[\"n\"]\n",
    "        data_final[\"number of attributes\"] = data[\"n_var\"]\n",
    "        data_final[\"dimensionality\"] = float(data[\"n_var\"]) / float(data[\"n\"])\n",
    "        num_cat = 0\n",
    "        num_num = 0\n",
    "        for key_types in data[\"types\"].keys():\n",
    "            if key_types == \"NUM\":\n",
    "                num_num+=data[\"types\"][key_types]\n",
    "            else:\n",
    "                num_cat+=data[\"types\"][key_types]\n",
    "        data_final[\"number of nominal\"] = num_cat\n",
    "        data_final[\"number of numeric\"] = num_num\n",
    "        data_final[\"percentage of nominal\"] = num_cat / float(data[\"n_var\"])\n",
    "        data_final[\"percentage of numeric\"] = num_num / float(data[\"n_var\"])\n",
    "        \n",
    "        #missing\n",
    "        data_final[\"missing attribute count\"] = data[\"n_vars_with_missing\"]\n",
    "        data_final[\"missing attribute percentage\"] = float(data[\"n_vars_with_missing\"]) / float(data[\"n_var\"])\n",
    "        num_missing_values = []\n",
    "        ptg_missing_values = []\n",
    "        \n",
    "        numeric_final = {}\n",
    "        #numeric\n",
    "        means = []\n",
    "        for key in att_numeric.keys():\n",
    "            att_num_row = []\n",
    "            att_num_row.append(att_numeric[key].pop(\"ds_id\"))\n",
    "            att_num_row.append(key)\n",
    "#             att_numeric[key] = concat_values(att_numeric[key])\n",
    "            if len(att_num_header) <=2:\n",
    "                for k in att_numeric[key].keys():\n",
    "                    att_num_header.append(k)\n",
    "            for k in att_numeric[key].keys():\n",
    "                att_num_row.append(att_numeric[key][k])\n",
    "            att_num_set.append(att_num_row)\n",
    "            ####\n",
    "            final_num_row = []\n",
    "            means.append(att_numeric[key][\"mean\"])\n",
    "            num_missing_values.append(att_numeric[key][\"n_missing\"])\n",
    "            ptg_missing_values.append(att_numeric[key][\"p_missing\"])\n",
    "            numeric_final[\"dataset id\"] = data[\"name\"]\n",
    "            numeric_final[\"attribute name\"] = att_num_row[1]\n",
    "            numeric_final[\"number distinct values\"] = att_numeric[key][\"distinct_count_without_nan\"]\n",
    "            numeric_final[\"percentage distinct values\"] = float(att_numeric[key][\"distinct_count_without_nan\"]) / float(att_numeric[key][\"n\"])\n",
    "            numeric_final[\"percentage missing values\"] = att_numeric[key][\"p_missing\"]\n",
    "            numeric_final[\"mean\"] = att_numeric[key][\"mean\"]\n",
    "            numeric_final[\"standard deviation\"] = att_numeric[key][\"std\"]\n",
    "            numeric_final[\"minimum value\"] = att_numeric[key][\"min\"]\n",
    "            numeric_final[\"maximum value\"] = att_numeric[key][\"max\"]\n",
    "            numeric_final[\"range\"] = att_numeric[key][\"range\"]\n",
    "            numeric_final[\"coefficient of variance\"] = att_numeric[key][\"cv\"]\n",
    "            if len(final_num_set_header) == 0:\n",
    "                for final_key in numeric_final.keys():\n",
    "                        final_num_set_header.append(final_key)\n",
    "            for final_key in numeric_final.keys():\n",
    "                final_num_row.append(numeric_final[final_key])\n",
    "            final_num_set.append(final_num_row)\n",
    "            \n",
    "        if len(means) == 0:\n",
    "            means = [0]\n",
    "        means = np.array(means)\n",
    "        data_final[\"average of means\"] = np.average(means)\n",
    "        data_final[\"standard deviation of means\"] = np.std(means)\n",
    "        data_final[\"minimum number of means\"] = np.amin(means)\n",
    "        data_final[\"maximum number of means\"] = np.amax(means)\n",
    "        \n",
    "        #nominal    \n",
    "        nominal_final = {}\n",
    "        num_distinct = []  \n",
    "        for key in att_cat.keys():\n",
    "#             if key == \"<page title>\":\n",
    "#                 continue\n",
    "            att_cat_row = []\n",
    "            num_distinct.append(len(att_cat[key][\"value_counts\"].keys()))\n",
    "            att_cat_row.append(att_cat[key].pop(\"ds_id\"))\n",
    "            att_cat_row.append(key)\n",
    "            vcounts = []\n",
    "            pvcounts = []\n",
    "            string_values = \"\"\n",
    "            for vkey in att_cat[key][\"value_counts\"].keys():\n",
    "                vcounts.append(float(att_cat[key][\"value_counts\"][vkey]))\n",
    "                pvcounts.append(float(att_cat[key][\"value_counts\"][vkey]) / float(att_cat[key][\"n\"]))\n",
    "                if string_values != \"\":\n",
    "                    string_values = string_values + \"|\"\n",
    "                text = att_cat[key][\"value_counts\"][vkey]\n",
    "                text = ''.join(char for char in text if ord(char) < 128)\n",
    "                text = text.replace(\"~\",\"\").replace(\"\\n\",\" \")\n",
    "                text2 = vkey\n",
    "                text2 = ''.join(char for char in text2 if ord(char) < 128)\n",
    "                text2 = text2.replace(\"~\",\"\").replace(\"\\n\",\" \")\n",
    "                string_values = string_values + str(text2) + \" \" + str(text)\n",
    "            \n",
    "            if len(vcounts) ==0:\n",
    "                vcounts = [0]\n",
    "            if len(pvcounts) ==0:\n",
    "                pvcounts = [0]\n",
    "            vcounts = np.array(vcounts)\n",
    "            pvcounts = np.array(pvcounts)\n",
    "#             att_cat[key] = concat_values(att_cat[key])\n",
    "            if len(att_cat_header) <=2:\n",
    "                for k in att_cat[key].keys():\n",
    "                    att_cat_header.append(k)\n",
    "            for k in att_cat[key].keys():\n",
    "                att_cat_row.append(att_cat[key][k])\n",
    "            att_cat_set.append(att_cat_row)\n",
    "            #####\n",
    "            final_nom_row = []\n",
    "            num_missing_values.append(att_cat[key][\"n_missing\"])\n",
    "            ptg_missing_values.append(att_cat[key][\"p_missing\"])\n",
    "            nominal_final[\"dataset id\"] = data[\"name\"]\n",
    "            nominal_final[\"attribute name\"] = att_cat_row[1]\n",
    "            #############################33\n",
    "#             if ds_names[d] == \"www.best-deal-items.com\":\n",
    "#                 print(\"################THE KEY##############: \"+key)\n",
    "#                 print(att_cat[key])\n",
    "            #############################33333333\n",
    "            nominal_final[\"number distinct values\"] = att_cat[key][\"distinct_count_without_nan\"]\n",
    "            nominal_final[\"percentage distinct values\"] = float(att_cat[key][\"distinct_count_without_nan\"]) / float(att_cat[key][\"n\"])\n",
    "            nominal_final[\"percentage missing values\"] = att_cat[key][\"p_missing\"]\n",
    "            nominal_final[\"mean number of string values\"] = np.average(vcounts)\n",
    "            nominal_final[\"standard deviation number of string values\"] = np.std(vcounts)\n",
    "            nominal_final[\"minimum number of string values\"] = np.amin(vcounts)\n",
    "            nominal_final[\"maximum number of string values\"] = np.amax(vcounts)\n",
    "            nominal_final[\"median percentage of string values\"] = np.median(pvcounts)\n",
    "            nominal_final[\"standard deviation percentage of string values\"] = np.std(pvcounts)\n",
    "            nominal_final[\"minimum percentage of string values\"] = np.amin(pvcounts)\n",
    "            nominal_final[\"maximum percentage of string values\"] = np.amax(pvcounts)\n",
    "            nominal_final[\"string values\"] = string_values\n",
    "            if len(final_nom_set_header) == 0:\n",
    "                for final_key in nominal_final.keys():\n",
    "                        final_nom_set_header.append(final_key)\n",
    "            for final_key in nominal_final.keys():\n",
    "                final_nom_row.append(nominal_final[final_key])\n",
    "            final_nom_set.append(final_nom_row)\n",
    "                \n",
    "            \n",
    "        num_distinct = np.array(num_distinct)\n",
    "        data_final[\"average number of distintc values\"] = np.average(num_distinct)\n",
    "        data_final[\"standard deviation of distintc values\"] = np.std(num_distinct)\n",
    "        data_final[\"minimum number of distintc values\"] = np.amin(num_distinct)\n",
    "        data_final[\"maximum number of distintc values\"] = np.amax(num_distinct)\n",
    "        \n",
    "        ##missing\n",
    "        num_missing_values = np.array(num_missing_values)\n",
    "        ptg_missing_values = np.array(ptg_missing_values)\n",
    "        data_final[\"average number of missing values\"] = np.average(num_missing_values)\n",
    "        data_final[\"standard deviation of missing values\"] = np.std(num_missing_values)\n",
    "        data_final[\"minimum number of missing values\"] = np.amin(num_missing_values)\n",
    "        data_final[\"maximum number of missing values\"] = np.amax(num_missing_values)\n",
    "        data_final[\"average number of percentage missing values\"] = np.average(ptg_missing_values)\n",
    "        data_final[\"standard deviation of percentage missing values\"] = np.std(ptg_missing_values)\n",
    "        data_final[\"minimum number of percentage missing values\"] = np.amin(ptg_missing_values)\n",
    "        data_final[\"maximum number of percentage missing values\"] = np.amax(ptg_missing_values)\n",
    "        \n",
    "        if len(final_ds_set_header) ==0:\n",
    "            for final_key in data_final.keys():\n",
    "                final_ds_set_header.append(final_key)    \n",
    "        for final_key in data_final.keys():\n",
    "            final_ds_row.append(data_final[final_key])\n",
    "        \n",
    "        final_ds_set.append(final_ds_row)\n",
    "        \n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame(final_ds_set, columns=final_ds_set_header)\n",
    "    df.to_csv(\"output/example/\"+\"ds2.csv\", index=False,sep=\"~\")\n",
    "    df = pd.DataFrame(final_nom_set, columns=final_nom_set_header)\n",
    "    df.to_csv(\"output/example/\"+\"attr_cat2.csv\", index=False,sep=\"~\")\n",
    "    df = pd.DataFrame(final_num_set, columns=final_num_set_header)\n",
    "    df.to_csv(\"output/example/\"+\"attr_num2.csv\", index=False,sep=\"~\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs =get_dirs(\"./challenge/output_monitor2/example/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddca7f46b9945fc90c327cb4ae30849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Summarize dataset', max=14.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7758bea048dc47538427f4b06f64d933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Render JSON', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "monitor_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba1065bacff4302abe15b488fe774fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Summarize dataset', max=21.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0819122d11024deb814a090ba31dbf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Render JSON', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "monitor_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e189743503a4e4c993b2c05899a3503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Summarize dataset', max=19.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971e0f7be3124549994eb693fb7c625a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Render JSON', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "monitor_3\n"
     ]
    }
   ],
   "source": [
    "write_csv(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "##extract metafeatures and store it as json \n",
    "ids = [1,3,6,8,10,12,14,16,17,18,19,20,22,23,24,25,28,29,30,32,34,36,40,41,43,44,55,56,59,150,151,164,171,172,179,180,183,185,187,189,196,197,203,205,208,211,212,216,222,224,225,227,230,232,285,287,300,301,307,308,313,315,327,328,329,343,354,375,378,381,382,451,470,473,477,480,483,488,489,492,493,503,505,507,510,511,534,537,538,541,542,544,552,554,561,562,563,567,568,570,571,573,575,576,577,578,666,679,686,1027,1029,1030,1035,1036,1037,1038,1040,1042,1044,1045,1049,1050,1051,1053,1054,1056,1059,1063,1064,1065,1066,1067,1069,1070,1071,1075,1089,1090,1091,1093,1094,1095,1096,1097,1098,1099,1111,1114,1119,1168,1461,1462,1463,1464,1465,1466,1470,1471,1472,1476,1477,1478,1481,1483,1487,1489,1490,1495,1497,1499,1500,1501,1502,1503,1504,1505,1509,1510,1511,1516,1517,1518,1519,1520,1523,1524,1525,1526,1557,1567,1590,1596,1597,4135,4532,4534,4544,4548,4549,4552,4563,4675,6331]\n",
    "\n",
    "def concat_values(json_data):\n",
    "    for key in json_data.keys():\n",
    "        if key == \"value_counts\" or key == \"types\":\n",
    "            concat = \"\"\n",
    "            for k in json_data[key].keys():\n",
    "                if concat != \"\":\n",
    "                    concat = concat + \"|\"\n",
    "                concat = concat + str(k) + \" \" + str(json_data[key][k])\n",
    "            json_data[key] = concat\n",
    "    return json_data\n",
    "\n",
    "ds_set = []\n",
    "ds_header = [\"ds_id\",\"ds_name\"]\n",
    "att_cat_set = []\n",
    "att_cat_header = [\"ds_id\",\"att_name\"]\n",
    "att_num_set = []\n",
    "att_num_header = [\"ds_id\",\"att_name\"]\n",
    "\n",
    "for i in ids:\n",
    "    with open(\"./mf_extracted/\"+str(i)+'.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    att_numeric = data.pop(\"numeric\")\n",
    "    att_cat = data.pop(\"nominal\")\n",
    "    data = concat_values(data)\n",
    "    del data[\"CAT\"]\n",
    "    del data[\"BOOL\"]\n",
    "    del data[\"NUM\"]\n",
    "    del data[\"DATE\"]\n",
    "    del data[\"URL\"]\n",
    "    del data[\"COMPLEX\"]\n",
    "    del data[\"PATH\"]\n",
    "    del data[\"FILE\"]\n",
    "    del data[\"IMAGE\"]\n",
    "    del data[\"UNSUPPORTED\"]\n",
    "    data[\"ds_id\"] = i\n",
    "    \n",
    "    ds_row = []\n",
    "    att_cat_row = []\n",
    "    att_num_row = []\n",
    "    \n",
    "    ds_row.append(data.pop(\"ds_id\"))\n",
    "    ds_row.append(data.pop(\"name\"))\n",
    "    if len(ds_header) <= 2:\n",
    "        for key in data.keys():\n",
    "            ds_header.append(key)\n",
    "    for key in data.keys():\n",
    "        ds_row.append(data[key])\n",
    "    ds_set.append(ds_row)\n",
    "    \n",
    "    for key in att_numeric.keys():\n",
    "        att_num_row.append(att_numeric[key].pop(\"ds_id\"))\n",
    "        att_num_row.append(key)\n",
    "        att_numeric[key] = concat_values(att_numeric[key])\n",
    "        if len(att_num_header) <=2:\n",
    "            for k in att_numeric[key].keys():\n",
    "                att_num_header.append(k)\n",
    "        for k in att_numeric[key].keys():\n",
    "            att_num_row.append(att_numeric[key][k])\n",
    "        att_num_set.append(att_num_row)\n",
    "        att_num_row = []\n",
    "    \n",
    "    for key in att_cat.keys():\n",
    "        att_cat_row.append(att_cat[key].pop(\"ds_id\"))\n",
    "        att_cat_row.append(key)\n",
    "        att_cat[key] = concat_values(att_cat[key])\n",
    "        if len(att_cat_header) <=2:\n",
    "            for k in att_cat[key].keys():\n",
    "                att_cat_header.append(k)\n",
    "        for k in att_cat[key].keys():\n",
    "            att_cat_row.append(att_cat[key][k])\n",
    "        att_cat_set.append(att_cat_row)\n",
    "        att_cat_row = []\n",
    "        \n",
    "df = pd.DataFrame(ds_set, columns=ds_header)\n",
    "df.to_csv(\"output/\"+\"ds2.csv\", index=False)\n",
    "df = pd.DataFrame(att_cat_set, columns=att_cat_header)\n",
    "df.to_csv(\"output/\"+\"attr_cat2.csv\", index=False)\n",
    "df = pd.DataFrame(att_num_set, columns=att_num_header)\n",
    "df.to_csv(\"output/\"+\"attr_num2.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
