{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataset with ~80% train, ~20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from step3 import step3_train_test_split as ds_split\n",
    "\n",
    "# default values\n",
    "train_mask = None\n",
    "test_mask = None\n",
    "neg_sample = 2\n",
    "strategy = \"random\"\n",
    "create_new_split = False\n",
    "word_embedding_encoding = \"FASTTEXT\"\n",
    "path_setup = None\n",
    "dataset_name = \"openml_203ds_datasets_matching\"\n",
    "cross_v=-1\n",
    "\n",
    "def parameter_error(param_error,value):\n",
    "    print(\"Encounter error in parameter {}, default value: {} will be used \".format(param_error,value))\n",
    "    \n",
    "def load_env(ds_name=None,ns=None,st=None,sp=None,we=None,cv=-1): \n",
    "    global dataset_name\n",
    "    global neg_sample\n",
    "    global strategy\n",
    "    global create_new_split\n",
    "    global word_embedding_encoding\n",
    "    global train_mask\n",
    "    global test_mask\n",
    "    global path_setup\n",
    "    global cross_v\n",
    "    \n",
    "    cross_v = cv\n",
    "\n",
    "    #see dataset_setup to config parametrs for split data\n",
    "    if ds_name == None or not str(ds_name): \n",
    "        parameter_error(\"dataset_name\",dataset_name)\n",
    "    else:\n",
    "        dataset_name = ds_name\n",
    "        \n",
    "    if ns == None or not int(ns) or ns < 0: \n",
    "        parameter_error(\"neg_sample\",neg_sample)\n",
    "    else:\n",
    "        neg_sample = ns\n",
    "        \n",
    "    if st == None or not str(st) or st not in [\"isolation\",\"random\"]:\n",
    "        parameter_error(\"strategy\",strategy)\n",
    "    else:\n",
    "        strategy = st\n",
    "    if sp == None:\n",
    "        parameter_error(\"create_new_split\",create_new_split)\n",
    "    else:\n",
    "        create_new_split = sp\n",
    "    if we == None or not str(we) or we not in [\"BERT\",\"FASTTEXT\"]:\n",
    "        parameter_error(\"word_embedding_encoding\",word_embedding_encoding)\n",
    "    else:\n",
    "        word_embedding_encoding = we\n",
    "\n",
    "    print(\"Values to load\")\n",
    "    print(\"dataset_name=\"+dataset_name)\n",
    "    print(\"neg_sample= \"+str(neg_sample))\n",
    "    print(\"strategy= \"+strategy)\n",
    "    print(\"create_new_split= \"+str(create_new_split))\n",
    "    print(\"word_embedding_encoding= \"+word_embedding_encoding)    \n",
    "    print(\"cross_v= \"+str(cross_v))    \n",
    "\n",
    "    if cross_v < 0:\n",
    "        if create_new_split:\n",
    "            print(\"Creating simple train/test splits...\")\n",
    "            path_setup = ds_split.split_ds(dataset_name,strategy,neg_sample)\n",
    "        else:\n",
    "            path_setup = dataset_name+\"/\"+strategy+\"/\"+str(neg_sample)\n",
    "        \n",
    "        train_mask = pd.read_csv(\"./datasets/\"+path_setup+\"/train.csv\").to_numpy()\n",
    "        test_mask = pd.read_csv(\"./datasets/\"+path_setup+\"/test.csv\").to_numpy()\n",
    "    \n",
    "    else:\n",
    "        if cross_v == 0:\n",
    "            print(\"Creating cross validation splits...\")\n",
    "            path_setup = ds_split.split_ds(dataset_name,strategy,neg_sample,True)\n",
    "        else:\n",
    "            path_setup = dataset_name+\"/\"+strategy+\"/\"+str(neg_sample)+\"/cv\"\n",
    "            \n",
    "        \n",
    "        train_mask = pd.read_csv(\"./datasets/\"+path_setup+\"/\"+str(cross_v)+\"/train.csv\").to_numpy()\n",
    "        test_mask = pd.read_csv(\"./datasets/\"+path_setup+\"/\"+str(cross_v)+\"/test.csv\").to_numpy()\n",
    "    \n",
    "    \n",
    "\n",
    "    #info about split\n",
    "    train_positive = np.array([x for x in train_mask if x[2]==1])\n",
    "    test_positive = np.array([x for x in test_mask if x[2]==1])\n",
    "    print(\"Dataset splits loaded\")\n",
    "    print(\"Train samples: \"+str(len(train_mask)) + \" Test samples: \"+str(len(test_mask)))\n",
    "    print(\"Train positive samples: \"+str(len(train_positive)) + \" Test positive samples: \"+str(len(test_positive)))\n",
    "    load_dgl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read graph of metafeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "map_ds = None\n",
    "map_reverse_ds_order = None\n",
    "def load_graph():\n",
    "    global map_ds\n",
    "    global map_reverse_ds_order\n",
    "    \n",
    "    if word_embedding_encoding == \"FASTTEXT\":\n",
    "        g_x = nx.read_gpickle(\"./word_embeddings/encoded_fasttext.gpickle\")\n",
    "    if word_embedding_encoding == \"BERT\":\n",
    "        g_x = nx.read_gpickle(\"./word_embeddings/encoded_bert.gpickle\")\n",
    "\n",
    "    ds_order = 0\n",
    "    for x,n in sorted(g_x.nodes(data=True)):\n",
    "        t = n['tipo']\n",
    "        if t == \"dataset\":\n",
    "            n['tipo'] = 0\n",
    "        if t == \"feature dataset\":\n",
    "            n['tipo'] = 1\n",
    "        if t == \"literal dataset\":\n",
    "            n['tipo'] = 2\n",
    "        if t == \"attribute\":\n",
    "            n['tipo'] = 3\n",
    "        if t == \"feature attribute\":\n",
    "            n['tipo'] = 4\n",
    "        if t == \"literal attribute\":\n",
    "            n['tipo'] = 5  \n",
    "        n['ds_order']=ds_order\n",
    "        ds_order+=1\n",
    "\n",
    "    datasets = [x for (x,y) in g_x.nodes(data=True) if y['tipo']==0]\n",
    "    ds_order = [y['ds_order'] for x,y in g_x.nodes(data=True) if y['tipo']==0]\n",
    "    map_ds = dict(zip(datasets,ds_order))\n",
    "    map_reverse_ds_order = dict(zip(ds_order,datasets))\n",
    "    map_ds['DS_1']\n",
    "\n",
    "    for mask in train_mask:\n",
    "        mask[0] = map_ds[\"DS_\"+str(mask[0])]\n",
    "        mask[1] = map_ds[\"DS_\"+str(mask[1])]\n",
    "        if mask[2] == 0:\n",
    "            mask[2] = -1\n",
    "    for mask in test_mask:\n",
    "        mask[0] = map_ds[\"DS_\"+str(mask[0])]\n",
    "        mask[1] = map_ds[\"DS_\"+str(mask[1])]\n",
    "    \n",
    "    return g_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export graph to deep graph library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "#convert from networkx to graph deep library format\n",
    "g = None\n",
    "def load_dgl():\n",
    "    global g\n",
    "    g_x = load_graph()\n",
    "    g = dgl.DGLGraph()\n",
    "    g.from_networkx(g_x,node_attrs=['tipo','vector','ds_order'], edge_attrs=None)\n",
    "    print(\"Meta-feature graph from datasets loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy based on thresholds of distance (e.g. cosine > 0.8 should be a positive pair)\n",
    "def threshold_acc(model, g, features, mask,loss,print_details=False,threshold_dist=0.2,threshold_cos=0.8):\n",
    "    indices = []\n",
    "    \n",
    "    #mask = np.array([x for x in mask if x[2]==1])\n",
    "    \n",
    "    z1, z2 = model(g,features,mask[:,0],mask[:,1])\n",
    "    \n",
    "    #dist() | m - dist()\n",
    "    if loss == \"ContrastiveLoss\" or loss == \"Euclidean\":\n",
    "        pdist = th.nn.PairwiseDistance(p=2)        \n",
    "        result = pdist(z1,z2)\n",
    "        for i in range(len(result)):\n",
    "            r = result[i]\n",
    "            if r.item() <= threshold_dist:\n",
    "                indices.append(1.0)\n",
    "            else:\n",
    "                indices.append(0.0)          \n",
    "        indices_tensor = th.tensor(indices)\n",
    "        labels_tensor = th.tensor(mask[:,2])\n",
    "        \n",
    "    #1 - cos() | max(0,cos() - m)\n",
    "    if loss == \"CosineEmbeddingLoss\":\n",
    "        cos = th.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        result = cos(z1,z2)\n",
    "        for i in range(len(result)):\n",
    "            r = result[i]\n",
    "            if r.item() >= threshold_cos:\n",
    "                indices.append(1.0)\n",
    "            else:\n",
    "                indices.append(0.0)\n",
    "        indices_tensor = th.tensor(indices)\n",
    "        labels_tensor = th.tensor(mask[:,2])\n",
    "    \n",
    "    if print_details:\n",
    "        positives = 0.0\n",
    "        negatives = 0.0\n",
    "        true_positives = 0.0\n",
    "        true_negatives = 0.0\n",
    "        false_positives = 0.0\n",
    "        false_negatives = 0.0\n",
    "        \n",
    "        for i in range(len(labels_tensor)):\n",
    "            prediction = indices_tensor[i].item()\n",
    "            label = labels_tensor[i].item()\n",
    "            if label == 0.0:\n",
    "                negatives+=1\n",
    "                if prediction == label:\n",
    "                    true_negatives+=1\n",
    "                else:\n",
    "                    false_positives+=1\n",
    "            else:\n",
    "                positives+=1\n",
    "                if prediction == label:\n",
    "                    true_positives+=1\n",
    "                else:\n",
    "                    false_negatives+=1\n",
    "        \n",
    "        #print confusion matrix            \n",
    "        print(\"\\t \\t \\t \\t ##########Labels##########\")\n",
    "        print(\"\\t \\t \\t \\t Similar \\t Not Similar\")\n",
    "        print(\"Prediction Similar: \\t \\t {} \\t \\t {}\".format(true_positives,false_positives))\n",
    "        print(\"Prediction Not Similar:  \\t {} \\t \\t {}\".format(false_negatives,true_negatives))\n",
    "        print(\"\\t \\t \\t \\t----------------------\")\n",
    "        print(\"\\t \\t \\t \\t{} \\t \\t {}\".format(positives,negatives))\n",
    "        print(\"\\nRecall/Sensitivity: \"+str(true_positives/positives))\n",
    "        print(\"Specificity/Selectivity: \"+str(true_negatives/negatives))\n",
    "        print(\"Accuracy: \"+str((true_positives + true_negatives) / len(labels_tensor)))\n",
    "        return (true_positives + true_negatives) / len(labels_tensor)\n",
    "    else:\n",
    "        correct = th.sum(indices_tensor == labels_tensor)\n",
    "        return correct.item() * 1.0 / len(labels_tensor)\n",
    "\n",
    "# Accuracy based on nearest neighboor (e.g. the nearest node should be a positive pair)\n",
    "def ne_ne_acc_isolation(model, g, features, mask,loss,print_details=False):\n",
    "\n",
    "    train_indices = np.unique(np.concatenate((train_mask[:,0],train_mask[:,1])))\n",
    "    train_pos_samples = np.array([x for x in train_mask if x[2]==1])    \n",
    "    train_pos_samples_indices = np.unique(np.concatenate((train_pos_samples[:,0],train_pos_samples[:,1])))\n",
    "    \n",
    "    mask_indices = np.unique(np.concatenate((mask[:,0],mask[:,1])))\n",
    "    mask_pos_samples = np.array([x for x in mask if x[2]==1])    \n",
    "    mask_pos_samples_indices = np.unique(np.concatenate((mask_pos_samples[:,0],mask_pos_samples[:,1])))\n",
    "    mask_pos_samples_indices = np.array([x for x in mask_pos_samples_indices if x not in train_pos_samples_indices ])\n",
    "    \n",
    "    pos_samples = np.concatenate((train_pos_samples,mask_pos_samples))\n",
    "    pos_samples_indices = np.unique(np.concatenate((pos_samples[:,0],pos_samples[:,1])))\n",
    "    train_embeddings,mask_pos_samples_embeddings = model(g, features,train_pos_samples_indices,mask_pos_samples_indices)\n",
    "    \n",
    "    sum_accuracy = 0\n",
    "    for i in range(len(mask_pos_samples_indices)):\n",
    "        candidate = mask_pos_samples_embeddings[i]\n",
    "        #dist() | m - dist()\n",
    "        if loss == \"ContrastiveLoss\":\n",
    "            pdist = th.nn.PairwiseDistance(p=2)        \n",
    "            result = pdist(candidate,train_embeddings)\n",
    "            largest = False\n",
    "        #1 - cos() | max(0,cos() - m)\n",
    "        if loss == \"CosineEmbeddingLoss\":\n",
    "            thecos = th.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            result = thecos(candidate.reshape(1,len(candidate)),train_embeddings)\n",
    "            largest = True\n",
    "        \n",
    "        #we ignore the result of the vector with itself\n",
    "#         print(\"Candidate id: \" + str(mask_pos_samples_indices[i]))        \n",
    "        result_indices = th.topk(result, 2, largest=largest).indices\n",
    "        closest_node_index = th.tensor(train_pos_samples_indices)[result_indices]\n",
    "#         print(closest_node_index)\n",
    "#         print(\"all in mask\")\n",
    "#         print(mask_pos_samples)\n",
    "        \n",
    "#         check_relation_nodes = np.array([x for x in pos_samples \n",
    "        check_relation_nodes = np.array([x for x in mask_pos_samples \n",
    "                                         if (x[0]==mask_pos_samples_indices[i] and x[1] in closest_node_index) or \n",
    "                                         (x[1]==mask_pos_samples_indices[i] and x[0] in closest_node_index)])\n",
    "#         print(\"relations found: \")\n",
    "#         print(check_relation_nodes)\n",
    "        if len(check_relation_nodes) > 0:\n",
    "            sum_accuracy += 1\n",
    "    \n",
    "    return sum_accuracy / len(mask_pos_samples_indices)    \n",
    "\n",
    "# Accuracy based on nearest neighboor (e.g. the nearest node should be a positive pair)\n",
    "def ne_ne_acc_random(model, g, features, mask,loss,print_details=False):\n",
    "\n",
    "    train_indices = np.unique(np.concatenate((train_mask[:,0],train_mask[:,1])))\n",
    "    train_pos_samples = np.array([x for x in train_mask if x[2]==1])    \n",
    "    train_pos_samples_indices = np.unique(np.concatenate((train_pos_samples[:,0],train_pos_samples[:,1])))\n",
    "    \n",
    "    mask_indices = np.unique(np.concatenate((mask[:,0],mask[:,1])))\n",
    "    mask_pos_samples = np.array([x for x in mask if x[2]==1])    \n",
    "    mask_pos_samples_indices = np.unique(np.concatenate((mask_pos_samples[:,0],mask_pos_samples[:,1])))\n",
    "#     mask_pos_samples_indices = np.array([x for x in mask_pos_samples_indices if x not in train_pos_samples_indices ])\n",
    "    \n",
    "    pos_samples = np.concatenate((train_pos_samples,mask_pos_samples))\n",
    "    pos_samples_indices = np.unique(np.concatenate((pos_samples[:,0],pos_samples[:,1])))\n",
    "    pos_embeddings,mask_pos_samples_embeddings = model(g, features,pos_samples_indices,mask_pos_samples_indices)\n",
    "    \n",
    "    sum_accuracy = 0\n",
    "    for i in range(len(mask_pos_samples_indices)):\n",
    "        candidate = mask_pos_samples_embeddings[i]\n",
    "        #dist() | m - dist()\n",
    "        if loss == \"ContrastiveLoss\":\n",
    "            pdist = th.nn.PairwiseDistance(p=2)        \n",
    "            result = pdist(candidate,pos_embeddings)\n",
    "            largest = False\n",
    "        #1 - cos() | max(0,cos() - m)\n",
    "        if loss == \"CosineEmbeddingLoss\":\n",
    "            thecos = th.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            result = thecos(candidate.reshape(1,len(candidate)),pos_embeddings)\n",
    "            largest = True\n",
    "        \n",
    "        #we ignore the result of the vector with itself\n",
    "#         print(\"Candidate id: \" + str(mask_pos_samples_indices[i]))        \n",
    "        result_indices = th.topk(result, 2, largest=largest).indices\n",
    "        closest_node_index = th.tensor(pos_samples_indices)[result_indices]\n",
    "#         print(closest_node_index)\n",
    "#         print(\"all in mask\")\n",
    "#         print(mask_pos_samples)\n",
    "        \n",
    "#         check_relation_nodes = np.array([x for x in pos_samples \n",
    "        check_relation_nodes = np.array([x for x in pos_samples \n",
    "                                         if (x[0]==mask_pos_samples_indices[i] and x[1] in closest_node_index) or \n",
    "                                         (x[1]==mask_pos_samples_indices[i] and x[0] in closest_node_index)])\n",
    "#         print(\"relations found: \")\n",
    "#         print(check_relation_nodes)\n",
    "        if len(check_relation_nodes) > 0:\n",
    "            sum_accuracy += 1\n",
    "    \n",
    "    return sum_accuracy / len(mask_pos_samples_indices)    \n",
    "\n",
    "def confusion_matrix(model, g, features, mask,loss,threshold):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        acc = threshold_acc(model, g, features, mask,loss,print_details=True,threshold_dist=threshold,threshold_cos=threshold)\n",
    "        return acc\n",
    "        \n",
    "def evaluate(model, g, features, mask,loss):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        #naive way of testing accuracy \n",
    "        acc = threshold_acc(model, g, features, mask,loss)\n",
    "        #accuracy based on 1-NN \n",
    "        if strategy == \"isolation\":\n",
    "            acc2 = ne_ne_acc_isolation(model, g, features, mask,loss)\n",
    "        if strategy == \"random\":\n",
    "            acc2 = ne_ne_acc_random(model, g, features, mask,loss)\n",
    "        return acc,acc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "def train(training,iterations):\n",
    "    dur = []\n",
    "    max_acc = 0.0\n",
    "    ## create batchs for training\n",
    "    numb_splits = int(len(train_mask) / training.batch_splits) + 1\n",
    "    train_batch = np.array_split(train_mask,numb_splits)\n",
    "    \n",
    "    #specify number of threads for the training\n",
    "    #th.set_num_threads(2)\n",
    "    \n",
    "    for epoch in range(iterations):\n",
    "        #model train mode\n",
    "        training.net.train()\n",
    "        t0 = time.time()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        #forward_backward positive batch sample\n",
    "        for split in train_batch:\n",
    "            z1,z2 = training.net(g, g.ndata['vector'],split[:,0],split[:,1])\n",
    "            loss = training.loss(z1,z2, th.tensor(split[:,2]))\n",
    "            training.optimizer.zero_grad()\n",
    "            #loss.backward(retain_graph=True)\n",
    "            loss.backward()\n",
    "            training.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = epoch_loss / training.batch_splits\n",
    "\n",
    "        #runtime\n",
    "        t = time.time() - t0\n",
    "        dur.append(t)\n",
    "        \n",
    "        #total time accumulation for this model\n",
    "        training.runtime_seconds+=t\n",
    "        \n",
    "        #accuracy\n",
    "        acc,acc2 = evaluate(training.net, g, g.ndata['vector'], test_mask,training.loss_name)\n",
    "        \n",
    "        #create log\n",
    "        output = {}\n",
    "        output['epoch'] = training.epochs_run\n",
    "        output['loss'] = float('%.5f'% (epoch_loss))\n",
    "        output['acc'] = float('%.5f'% (acc))\n",
    "        output['acc2'] = float('%.5f'% (acc2))\n",
    "        output['time_epoch'] = float('%.5f'% (np.mean(dur)))\n",
    "        output['time_total'] = float('%.5f'% (training.runtime_seconds))\n",
    "        training.log.append(output)\n",
    "        training.epochs_run+=1\n",
    "        print(str(output))\n",
    "        \n",
    "        ##save best model and results found so far\n",
    "        if acc2 + acc > max_acc:\n",
    "            print(\"Best model found so far...\")\n",
    "            training.save_state(path_setup+\"/best\")\n",
    "            max_acc = acc2 + acc \n",
    "        \n",
    "    #save final model state and final results\n",
    "    training.save_state(path_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config and run training\n",
    "### NN architectures: \n",
    "{<br>\n",
    "    \"0\": \"Bert_300\", <br>\n",
    "    \"1\": \"Bert_300_300_200\", <br>\n",
    "    \"2\": \"Bert_768\", <br>\n",
    "    \"3\": \"Fasttext3GCN_300\" <br>\n",
    "    \"4\": \"Fasttext_150\", <br>\n",
    "    \"5\": \"Fasttext_150_150_100\", <br>\n",
    "    \"6\": \"Fasttext_300\" <br>\n",
    "}\n",
    "### Loss functions: \n",
    "{<br>\n",
    "    \"0\": \"ContrastiveLoss\", <br>\n",
    "    \"1\": \"CosineEmbeddingLoss\", <br>\n",
    "}\n",
    "### Example to define architecture and loss\n",
    "<b>from step3 import step3_gcn_nn_concatenate as gcn_nn</b> <br>\n",
    "<b>from step3 import step3_gcn_loss as gcn_loss</b> <br>\n",
    "print(gcn_nn.get_options()) #list of options<br>\n",
    "print(gcn_loss.get_options()) #list of options<br>\n",
    "\n",
    "### Load training class to save/load/train experiments:\n",
    "<b>from step3 import step3_gcn_train as gcn_train</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from step3 import step3_gcn_nn_concatenate as gcn_nn\n",
    "# from step3 import step3_gcn_loss as gcn_loss\n",
    "# from step3 import step3_gcn_training as gcn_training\n",
    "#load_env(ns=None,st=None,sp=None,we=None)\n",
    "\n",
    "\n",
    "# #load model from path\n",
    "# training = gcn_training.Training()\n",
    "# training.load_state(path=\"./models/[file_name].pt\")\n",
    "# train(training,iterations=N)\n",
    "\n",
    "# #train new model and specify parameters\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(),  #_of_option for NN architecture\n",
    "#             batch_splits= ,#_of_sets(this will (give dataset / batch_splits) size of batch\n",
    "#             lr= , #learning rate for training (e.g. 1e-3 )\n",
    "#             loss_name=gcn_loss.get_option_name() #_of_option for loss ,\n",
    "#             loss_parameters=) #loss function parameters separated by '+' e.g. for cosine and contrastive \"0.0+mean\" \n",
    "# train(training,iterations=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training = gcn_training.Training()\n",
    "# training.load_state(path=\"./models/random/2/net_name:Fasttext_300|batch_splits:28.0000|lr:0.0010|loss_name:ContrastiveLoss|loss_parameters:0.7+mean.pt\")\n",
    "#train(training,iterations=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "cv_logs = []\n",
    "def cv_training(training,it):\n",
    "    train(training,iterations=it)\n",
    "    return training.log\n",
    "\n",
    "def cross_validation(training,iterations=1):\n",
    "    global cv_logs\n",
    "    training_copy = None\n",
    "    for i in range(10):\n",
    "        load_env(ds_name=dataset_name,ns=neg_sample,st=strategy,sp=create_new_split,we=word_embedding_encoding,cv=i)\n",
    "        training_copy = copy.deepcopy(training)\n",
    "        cv_logs.append(cv_training(training_copy,iterations))\n",
    "    outdir = \"./results/\"+training_copy.gen_path\n",
    "    if not os.path.exists(outdir):\n",
    "        Path(outdir).mkdir(parents=True, exist_ok=True)    \n",
    "    file_out = open(outdir+\"/tmp_cv_result.txt\",'w') \n",
    "    file_out.writelines(str(cv_logs))\n",
    "    file_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
